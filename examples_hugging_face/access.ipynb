{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iJUj0TiKtGJS"
   },
   "source": [
    "# Access GrandTour Data using HuggingFace ðŸ¤—\n",
    "Â© 2025 ETH Zurich\n",
    " \n",
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/leggedrobotics/grand_tour_dataset/blob/main/examples/%5B0%5D_Accessing_GrandTour_Data.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "> GrandTour data is avaialable in two formats, hosted on two platforms:\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th style=\"padding:10px;text-align:left;\">Format</th>\n",
    "    <th style=\"padding:10px;text-align:left;\"> </th>\n",
    "    <th style=\"padding:10px;text-align:left;\">Hosted&nbsp;on</th>\n",
    "    <th style=\"padding:10px;text-align:left;\"> </th>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/leggedrobotics/grand_tour_dataset/main/assets/ros-logo.png\"  height=\"30\" alt=\"ROS logo\"></td>\n",
    "    <td style=\"padding-left:15px;\"><a href=\"https://wiki.ros.org/rosbag\">ROS&nbsp;Bags</a></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/leggedrobotics/grand_tour_dataset/main/assets/rsl-logo.png\"  height=\"30\" alt=\"RSL logo\"></td>\n",
    "    <td style=\"padding-left:15px;\">Kleinkram</td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "  <tr>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/leggedrobotics/grand_tour_dataset/main/assets/zarr-logo.png\" height=\"40\" alt=\"Zarr logo\"></td>\n",
    "    <td style=\"padding-left:15px;\"><a href=\"https://zarr.dev/\">ZARR</a></td>\n",
    "    <td><img src=\"https://raw.githubusercontent.com/leggedrobotics/grand_tour_dataset/main/assets/hf-logo.png\"  height=\"30\" alt=\"Hugging Face logo\"></td>\n",
    "    <td style=\"padding-left:15px;\">HuggingFace</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "> This notebook explains how to download the zarr/png converted dataset hosted on Huggingface.\n",
    ">\n",
    "> \n",
    "> ðŸ’¡ Please refer to the `examples_hugging_face/explore.ipynb` on how to use the data.\n",
    " \n",
    "## Downloading\n",
    "> We provide the entire dataset on HuggingFace in `.zarr`, `.png`, and `.yaml` format.\n",
    "> \n",
    "> To avoid checking in +1M individual files on the HuggingHub, we created a tar-ball `.tar` for each topic per mission.\n",
    "\n",
    "> HuggingFace has an easy-to-use Python download API called `huggingface_hub`.\n",
    "> It is possible to download directly from the [GrandTour HuggingFace repo UI](https://huggingface.co/leggedrobotics), but we strongly reccomend making use of `huggingface_hub`, as it manages caching files, interrupted downloads and smart fetching of updated files.\n",
    "\n",
    "> First, install `huggingface_hub` which requires you to  have an HuggingFace account. You can create one for free at [huggingface.co](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q huggingface_hub # Should be already installed when following the README.md and uv installation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, login using the cli. This will store authentication tokens on your PC and allow you to use the API to download data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your notebook isn't able to take input from the command line, run this in a local terminal instead\n",
    "! huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now you can download an a mission of your choice. The next tutorial - _[1] Exploring GrandTour Data_ - uses 2024-10-01-11-29-55, so we will donwload it here in anticipation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Specify the mission you want to download.\n",
    "mission = \"2024-10-01-11-29-55\"\n",
    "\n",
    "# Download the full dataset\n",
    "allow_patterns = [f\"*\"]\n",
    "\n",
    "# Download all data from a single mission\n",
    "allow_patterns = [f\"{mission}/*\"]\n",
    "\n",
    "# Download a specific topic\n",
    "# topic = \"alphasense_front_center\"\n",
    "# allow_patterns = [f\"{mission}/*{topic}*\", f\"{mission}/*.yaml\"]\n",
    "\n",
    "\n",
    "# to only include a subset of the \n",
    "\n",
    "# If this is interuppted during download, simply re-run the block and huggingface_hub will resume the download without re-downloading the already downloaded files.\n",
    "hugging_face_data_cache_path = snapshot_download(repo_id=\"leggedrobotics/grand_tour_dataset\", allow_patterns=allow_patterns, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The downloaded data will be compressed into `.tar` files, and must be extracted before it can be used. We reccomend extracting to a destination of your choice outside the huggingface cache directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the destination directory\n",
    "dataset_folder = Path(\"~/grand_tour_dataset\").expanduser()\n",
    "dataset_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Print for confirmation\n",
    "print(f\"Data will be extracted to: {dataset_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define a `.tar` extractor helper function and extract the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "\n",
    "def move_dataset(cache, dataset_folder, allow_patterns=[\"*\"]):\n",
    "\n",
    "    def convert_glob_patterns_to_regex(glob_patterns):\n",
    "        regex_parts = []\n",
    "        for pat in glob_patterns:\n",
    "            # Escape regex special characters except for * and ?\n",
    "            pat = re.escape(pat)\n",
    "            # Convert escaped glob wildcards to regex equivalents\n",
    "            pat = pat.replace(r'\\*', '.*').replace(r'\\?', '.')\n",
    "            # Make sure it matches full paths\n",
    "            regex_parts.append(f\".*{pat}$\")\n",
    "        \n",
    "        # Join with |\n",
    "        combined = \"|\".join(regex_parts)\n",
    "        return re.compile(combined)\n",
    "    \n",
    "    pattern = convert_glob_patterns_to_regex(allow_patterns)\n",
    "    files = [f for f in Path(cache).rglob(\"*\") if pattern.match(str(f))]\n",
    "    tar_files = [f for f in files if f.suffix == \".tar\" ]\n",
    "    \n",
    "    for source_path in tar_files:\n",
    "        dest_path = dataset_folder / source_path.relative_to(cache)\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            with tarfile.open(source_path, \"r\") as tar:\n",
    "                tar.extractall(path=dest_path.parent)\n",
    "        except tarfile.ReadError as e:\n",
    "            print(f\"Error opening or extracting tar file '{source_path}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {source_path}: {e}\")\n",
    "    \n",
    "    other_files = [f for f in files if not f.suffix == \".tar\" and f.is_file()]\n",
    "    for source_path in other_files:\n",
    "        dest_path = dataset_folder / source_path.relative_to(cache)\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(source_path,dest_path)\n",
    "\n",
    "    print(f\"Moved data from {cache} to {dataset_folder} !\")\n",
    "\n",
    "\n",
    "move_dataset(hugging_face_data_cache_path, dataset_folder, allow_patterns=allow_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You should now be able to load the dataset in `.zarr` format an inspect the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr.storage\n",
    "\n",
    "store = zarr.storage.LocalStore(dataset_folder / mission / \"data\")\n",
    "root = zarr.group(store=store)\n",
    "\n",
    "print([k for k in root.keys()])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
